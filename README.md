# Multimodal-Mental-Health-Prediction
This project presents a Multimodal Mental Health Monitoring Chatbot that analyzes a user’s psychological state using three inputs: text, facial expressions, and voice. When the user begins a session, the system requests a 6-second camera check-in to capture baseline facial emotions. The chatbot then engages the user in normal text conversation, where the system continuously evaluates sentiment, cognitive distortions, stress cues, suicide risk, and conversation tone using NLP models. Additionally, the user can optionally keep their camera on during the chat for real-time emotional tracking or choose to record their voice, allowing the system to analyze speech rate, vocal stress, tone stability, and fatigue.

All three modalities—chat text, face, and speech—are fused to generate detailed mental-health insights, including depression level, anxiety level, stress level, emotion state, risk score, and personalized recommendations. The system also produces alerts, wellness scores, session summaries, and weekly reports to help users track changes over time. This multimodal approach creates a more accurate and human-like understanding of user well-being, making the project innovative, practical, and highly beneficial in real-world mental-health applications.
